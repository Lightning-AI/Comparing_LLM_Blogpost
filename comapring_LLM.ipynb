{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "from model_lit_llama import  LitLlamaPipeline\n",
    "from models_HF import CustomPipeline\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain  \n",
    "from langchain.prompts import PromptTemplate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\"Llama\",\n",
    "        \"GPT3.5\", \n",
    "        \"EleutherAI/gpt-j-6B\" ,\n",
    "        \"google/ul2\",\n",
    "        \"bigscience/bloomz-7b1\",\n",
    "        \"facebook/opt-iml-max-30b\" ,\n",
    "        \"google/flan-t5-xxl\" ,\n",
    "        \"bigscience/bloomz-7b1\",\n",
    "        \"EleutherAI/gpt-neox-20b\",\n",
    "        \"EleutherAI/pythia-12b-deduped\"\n",
    "]\n",
    "\n",
    "# If you decide to  use GPT provide a  your key:\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For lit-LLaMA you'll need to covert LLaMA weigths runing meta_weights_for_nano_model \n",
    "model_id = model_ids[0]\n",
    "\n",
    "if model_id == model_ids[0]:\n",
    "    from covert_llama_weights import meta_weights_for_nano_model\n",
    "    meta_weights_for_nano_model(\n",
    "        output_dir = Path(\"checkpoints/lit-llama\"),\n",
    "        ckpt_dir = Path(your_llama_checkpoint_path),\n",
    "        tokenizer_path = Path(your_tokenizer_path),\n",
    "        model_size=model_size,\n",
    "    )\n",
    "    llm = LitLlamaPipeline()\n",
    "elif model_id == model_ids[1]:\n",
    "    llm = OpenAI(temperature=.75)\n",
    "else:\n",
    "    llm = CustomPipeline(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template =  \"\"\"\n",
    "We have provided context information below.\n",
    "\n",
    "Elon Musk's Twitter misery seems to be delighting users on the social media platform, as he got stuck with a new screen name.\n",
    "The owner and CEO of Twitter has encountered the same problem as others have had before, and he must now seemingly go by the name \"Mr. Tweet\" for the foreseeable future.\n",
    "Musk inadvertently received the nickname from a lawyer while he was in court this week. He shared his misfortune with his millions of followers, and didn't receive much sympathy in return.\n",
    "Mr. Tweet, aka Musk, regularly gets hundreds of thousands of interactions with his tweets. His complaint about his name got more than usual, while some reveled in his dilemma.\n",
    "It's not the first time a celebrity has found themselves stuck with a Twitter name they didn't want. In November, singer Doja Cat called on Musk for help after she got stuck with an unusual name.\n",
    "\"I don't wanna be Christmas forever [Elon Musk] please help I've made a mistake,\" she wrote. Musk replied telling her they're working on it, but he also acknowledged it was \"pretty funny though.\"\n",
    "The irony that the owner and CEO of Twitter couldn't change his own name wasn't lost on many of his followers.\n",
    "\"Have you tried calling the help desk?\" Twitter user @TheChiefNerd replied.\n",
    "Musk's new screen name wasn't picked at random, though, as some explained how the joke came about.\n",
    "On January 23, long before Musk renamed himself, Silicon Valley journalist Teddy Schleifer explained: \"The lawyer who is cross-examining Elon Musk accidentally just called him 'Mr. Tweet' instead of 'Mr. Musk.' Elon says 'Mr. Tweet' is all good. 'That's probably an accurate description,'\" Schleifer wrote. Musk even clicked like on that tweet at the time.\n",
    "Musk was appearing in court during the Tesla shareholder trial. Investors are suing him, alleging that he committed securities fraud via a tweet in 2018.\n",
    "\"Mr. Tweet in the house...\" wrote Fox News contributor Joe Concha. Journalist Johnna Crider also approved of the new pseudonym. \"I personally think Mr. Tweet is better—has more personality as a nickname,\" she commented.\n",
    "The popular creator @iamchillpill joked that Musk would have to speak into a mirror to find help. \"Mr. Tweet please, let me be Elon again,\" they wrote.\n",
    "Online news outlet The Chainsaw didn't see the funny side though, and brought the name back to the ongoing litigation. \"Hey Mr. Tweet, how's the Tesla trial going?\" it wrote.\n",
    "Newsweek reached out to representatives of Musk and Tesla for comment on the ongoing trial.\n",
    "\n",
    "Using only this information, please answer the question: {text}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = PromptTemplate(input_variables=[ \"text\"], template=template)\n",
    "answer_chain = LLMChain(llm=llm , prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = [\"what’s Elon's new Twitter username?\",\n",
    "    \"why is it funny that he cannot change it?\",\n",
    "    \"make a joke about this\",\n",
    "    \"How did this get started?\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    answer_chain = LLMChain(llm=llm , prompt=prompt_template)\n",
    "    answer = answer_chain.run(question)\n",
    "    print(f\"\\n {answer} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1d1a5c79b143da8043a5a45d8e6cbc7d061ee79fad4c3bda183c9e6e328b611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
